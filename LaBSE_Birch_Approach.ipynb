{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4eeed13fb2a4acb8beedb04528ae617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05fb44b4ddd14688b3d333c07495ad26",
              "IPY_MODEL_40f33ab82ada4a2380672c4aed7ea83b",
              "IPY_MODEL_71b6899e1cac4dabb1dba675834692dc"
            ],
            "layout": "IPY_MODEL_f7fb94137ecd4bb19fb90e621da86953"
          }
        },
        "05fb44b4ddd14688b3d333c07495ad26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7025dd62de4ad891cb93b3aeb58d0b",
            "placeholder": "​",
            "style": "IPY_MODEL_c3b8de9b6a754e519c78513bfa97d957",
            "value": ""
          }
        },
        "40f33ab82ada4a2380672c4aed7ea83b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b074618639804cd6b732dcd9993d6822",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd55bffc32ef4e46a77b6ffd90e1628a",
            "value": 0
          }
        },
        "71b6899e1cac4dabb1dba675834692dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dccfe5e2139442848a2f5fc9f52bf61f",
            "placeholder": "​",
            "style": "IPY_MODEL_8e01c78d604647a4adc875bfd848b4e2",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "f7fb94137ecd4bb19fb90e621da86953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7025dd62de4ad891cb93b3aeb58d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b8de9b6a754e519c78513bfa97d957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b074618639804cd6b732dcd9993d6822": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bd55bffc32ef4e46a77b6ffd90e1628a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dccfe5e2139442848a2f5fc9f52bf61f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e01c78d604647a4adc875bfd848b4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LaBSE \n",
        "*   Paper: https://arxiv.org/pdf/2007.01852.pdf\n",
        "*   Blog: https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html \n",
        "*   Hugging face: https://huggingface.co/sentence-transformers/LaBSE\n",
        "\n",
        "BIRCH\n",
        "\n",
        "\n",
        "*   Paper: https://arxiv.org/abs/1903.10972\n",
        "*   Code Repository: https://github.com/castorini/birch"
      ],
      "metadata": {
        "id": "QOyy2yXtMG5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOUtNPmCtuhX",
        "outputId": "df38d01d-5f95-458e-d777-9fbd65b930a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/New_Work/Complete_Concept_Dataset.zip -d /content/\n",
        "!unzip /content/drive/MyDrive/New_Work/Complete_Test_Collection_without_RBR.zip -d /content/"
      ],
      "metadata": {
        "id": "UKpEedQVt7Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('/content/Train_Dataset/bangladesh')\n",
        "shutil.rmtree('/content/Train_Dataset/economy')\n",
        "shutil.rmtree('/content/Train_Dataset/education')\n",
        "shutil.rmtree('/content/Train_Dataset/entertainment')\n",
        "# shutil.rmtree('/content/Train_Dataset/health')\n",
        "shutil.rmtree('/content/Train_Dataset/international')\n",
        "shutil.rmtree('/content/Train_Dataset/kolkata')\n",
        "shutil.rmtree('/content/Train_Dataset/life-style')\n",
        "shutil.rmtree('/content/Train_Dataset/national')\n",
        "shutil.rmtree('/content/Train_Dataset/opinion')\n",
        "shutil.rmtree('/content/Train_Dataset/sports')\n",
        "shutil.rmtree('/content/Train_Dataset/technology')\n",
        "shutil.rmtree('/content/Train_Dataset/travel')\n",
        "shutil.rmtree('/content/Train_Dataset/we-are')\n",
        "shutil.rmtree('/content/Concept_Dataset/Old_Dataset_Concepts')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/Entertainment')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/International')\n",
        "# shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/health')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/kolkata')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/national')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/sports')\n",
        "shutil.rmtree('/content/Concept_Dataset/New_Dataset_Concepts/travel')"
      ],
      "metadata": {
        "id": "twoFi_cNt_s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_dir_path = \"/content/Concept_Dataset/New_Dataset_Concepts/health\""
      ],
      "metadata": {
        "id": "XMZ3Uu9QuEG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install weighted-levenshtein\n",
        "!pip install bnunicodenormalizer\n",
        "!pip install indic-nlp-library\n",
        "!pip install stopwordsiso\n",
        "!pip install nltk\n",
        "!pip install -U bnlp_toolkit\n",
        "!pip install git+https://github.com/riteshpanjwani/pyiwn@master#egg=pyiwn\n",
        "!pip install xlrd\n",
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "id": "4pYmd4bHuCuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # download tokenizer\n",
        "!wget https://storage.googleapis.com/ai4b-public-data/indicBERT_sentencepiece_tokenizer.py"
      ],
      "metadata": {
        "id": "TirH8BxQucGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stopwordsiso as stopwords\n",
        "import os\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as eng_stopwords\n",
        "import stopwordsiso as stopwords\n",
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "from bnlp import NLTKTokenizer\n",
        "import sklearn\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import scipy\n",
        "\n",
        "import ast\n",
        "import xlsxwriter\n",
        "import statistics\n",
        "\n",
        "from indicBERT_sentencepiece_tokenizer import IndicBERTSentencePieceTokenizer\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from copy import copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "a4eeed13fb2a4acb8beedb04528ae617",
            "05fb44b4ddd14688b3d333c07495ad26",
            "40f33ab82ada4a2380672c4aed7ea83b",
            "71b6899e1cac4dabb1dba675834692dc",
            "f7fb94137ecd4bb19fb90e621da86953",
            "6b7025dd62de4ad891cb93b3aeb58d0b",
            "c3b8de9b6a754e519c78513bfa97d957",
            "b074618639804cd6b732dcd9993d6822",
            "bd55bffc32ef4e46a77b6ffd90e1628a",
            "dccfe5e2139442848a2f5fc9f52bf61f",
            "8e01c78d604647a4adc875bfd848b4e2"
          ]
        },
        "id": "sNn1PuwuueMW",
        "outputId": "014b3a0c-b16c-4a92-8627-788e3c36cbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4eeed13fb2a4acb8beedb04528ae617"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/LaBSE')"
      ],
      "metadata": {
        "id": "cAgc21fgvqk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_file_paths=[]\n",
        "tr_docs=[]\n",
        "tr_dids=[]\n",
        "factory = IndicNormalizerFactory()\n",
        "normalizer = factory.get_normalizer(\"bn\", remove_nuktas=False)\n",
        "for dirpath, dirnames, filenames in os.walk(\"/content/Train_Dataset\"):\n",
        "  for file_name in filenames:\n",
        "    file_path = os.path.join(str(dirpath), str(file_name))\n",
        "    tr_file_paths.append(file_path)\n",
        "    fileObject = open(file_path, \"r+\", encoding='utf-8', errors='ignore')\n",
        "    text = fileObject.read()\n",
        "    text = normalizer.normalize(text)\n",
        "    tr_docs.append(text)\n",
        "    doc_num = int(file_path.split('/')[-1].replace('.txt',''))\n",
        "    tr_dids.append(doc_num)\n",
        "print(\"Number of documents in the corpus: \",len(tr_file_paths))"
      ],
      "metadata": {
        "id": "zIhJK0OdzabP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code to read ESA Concept dataset\n",
        "c_docs = []\n",
        "c_dids = []\n",
        "c_file_paths = []\n",
        "for dirpath, dirnames, filenames in os.walk(concept_dir_path):\n",
        "  for file_name in filenames:\n",
        "    # Get the absolute file path\n",
        "    file_path = os.path.join(str(dirpath), str(file_name))\n",
        "    c_file_paths.append(file_path)\n",
        "    # Create a file object.\n",
        "    fileObject = open(file_path, \"r+\", encoding='utf-8', errors='ignore')\n",
        "    # Extract the text from the file.\n",
        "    text = fileObject.read()\n",
        "    c_docs.append(text)\n",
        "    # using re.findall() we extract all the numbers from the string\n",
        "    temp = re.findall(r'\\d+', file_name)\n",
        "    c_doc_num = ''.join(temp)\n",
        "    c_dids.append(c_doc_num)\n",
        "\n",
        "print(\"Number of documents in the corpus: \",len(c_dids))"
      ],
      "metadata": {
        "id": "sidCmTrRzfe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class document_linearization:\n",
        "\tdef pre_process1(self,input_text):\n",
        "\t\t\"\"\"\n",
        "\t\tDescription: Performs text normalisation, removes punctuations and stop words and returns a list of tokens\n",
        "\t\t:param input_text:  The text which needs to be cleaned.\n",
        "\t\t:param stop_words: List of stopwords to be removed from the input text.\n",
        "\t\t:return: list of tokens\n",
        "\t\t\"\"\"\n",
        "\t\tstop_words = stopwords.stopwords(\"bn\").union(set(eng_stopwords.words('english')),\n",
        "                                                     {'একটা', 'তাঁহার', 'দিয়া', 'বলিয়া', 'লইয়া', 'আসিয়া', 'লাগিল',\n",
        "                                                      'কহিল', 'আসিয়া', 'তাহাকে', 'করিল', 'কহিলেন', 'কোনটি', 'ধরনের',\n",
        "                                                      'বলিলেন', 'করিত', 'চলিয়া', 'ধরিয়া', 'পড়িয়া', 'করিবার', 'হইত',\n",
        "                                                      'নহে', 'ফিরিয়া', 'কেহ', 'বলিতে', 'রহিল', 'পড়িল', 'উঠিল',\n",
        "                                                      'বাহির', 'দেখিয়া', 'উঠিয়া', 'ফিরিয়া', 'অবশেষে', 'কিছুতেই',\n",
        "                                                      'তেমনি', 'তোমাকে', 'নে', 'মাঝে', 'সকল', 'অত্যন্ত', 'একটু',\n",
        "                                                      'একেবারে', 'এক', 'একটা', 'একদিন', 'কথা', 'তারই', 'হয়ে', 'হওয়ায়',\n",
        "                                                      'রয়েছে', 'এমনকি', 'একজন', 'দিয়েছেন', 'একটা', 'হয়নি', 'সাথে',\n",
        "                                                      'হয়েই', 'দিয়ে', 'কেমনে', 'করিয়ে', 'তোরা', 'জন্যে', 'পেয়ে',\n",
        "                                                      'পাওয়া', 'তোর', 'ছাড়া', 'ছাড়াও', 'হওয়ার', 'তোমাদের', 'চেয়ে',\n",
        "                                                      'কথা', 'জানিয়েছে', 'মত', 'অর্থাৎ', 'গিয়েছে', 'জাানিয়ে', 'হয়েছে',\n",
        "                                                      'হিসেবে', 'হওয়া', 'এলো', 'করায়', 'তাঁহারা', 'দেওয়ার', 'হইয়া',\n",
        "                                                      'হয়েছেন', 'তোদের', 'অর্ধভাগে', 'তিনই', 'এসো', 'দেয়', 'এক', 'যায়',\n",
        "                                                      'দিয়েছে', 'চায়', 'হয়েছিল', 'তুই', 'হয়তো', 'হৈতে', 'অনুযায়ী',\n",
        "                                                      'কয়েকটি', 'পাঁচ', 'করিয়া', 'সময়', 'থাকায়'})\n",
        "\n",
        "\t\t# Perform text normalisation\n",
        "\t\t# Reference: https://nbviewer.jupyter.org/url/anoopkunchukuttan.github.io/indic_nlp_library/doc/indic_nlp_examples.ipynb#Text-Normalization\n",
        "\t\tfactory = IndicNormalizerFactory()\n",
        "\t\tnormalizer = factory.get_normalizer(\"bn\", remove_nuktas=False)\n",
        "\t\toutput_text=normalizer.normalize(input_text)\n",
        "\n",
        "\t\t# Remove punctuation from the text.\n",
        "\t\ttext = output_text.strip()\n",
        "\t\ttext = text.replace(\"’\", \"\")\n",
        "\t\ttext = text.replace(\"‘\", \"\")\n",
        "\t\ttext = text.replace('”', \"\")\n",
        "\t\ttext = text.replace(\"“\", \"\")\n",
        "\n",
        "\t\t# Some of the punctuation symbols were obtained from here: https://github.com/paul-pias/Text-Preprocessing-in-Bangla-and-English\n",
        "\t\tclean_text = re.sub(r\"[!\\\"”#$%&’()*+,-./:;<=>?@\\[\\]^_`{|}~–—\\'।॥]+\\ *\", ' ', text)\n",
        "\t\tclean_text = re.sub(r\"[-—]+\\ *\", ' ', clean_text)\n",
        "\t\tclean_text = re.sub(r\"\\n\", '', clean_text)\n",
        "\t\tclean_text = \" \".join(clean_text.split())\n",
        "\n",
        "\t\t# tokenize the cleaned text\n",
        "\t\tterms = clean_text.split(' ')\n",
        "\t\t# Remove stop words\n",
        "\t\tfiltered_sentence = []\n",
        "\t\tfor w in terms:\n",
        "\t\t\t# if w not in stop_words:\n",
        "\t\t\t\tfiltered_sentence.append(w)\n",
        "\t\tterms = filtered_sentence\n",
        "\t\treturn terms\n",
        "\t\n",
        "\tdef pre_process(self,input_text):\n",
        "\t\t\"\"\"\n",
        "\t\tDescription: Performs text normalisation, removes punctuations and stop words and returns a list of tokens\n",
        "\t\t:param input_text:  The text which needs to be cleaned.\n",
        "\t\t:param stop_words: List of stopwords to be removed from the input text.\n",
        "\t\t:return: list of tokens\n",
        "\t\t\"\"\"\n",
        "\t\tstop_words = stopwords.stopwords(\"bn\").union(set(eng_stopwords.words('english')),\n",
        "                                                     {'একটা', 'তাঁহার', 'দিয়া', 'বলিয়া', 'লইয়া', 'আসিয়া', 'লাগিল',\n",
        "                                                      'কহিল', 'আসিয়া', 'তাহাকে', 'করিল', 'কহিলেন', 'কোনটি', 'ধরনের',\n",
        "                                                      'বলিলেন', 'করিত', 'চলিয়া', 'ধরিয়া', 'পড়িয়া', 'করিবার', 'হইত',\n",
        "                                                      'নহে', 'ফিরিয়া', 'কেহ', 'বলিতে', 'রহিল', 'পড়িল', 'উঠিল',\n",
        "                                                      'বাহির', 'দেখিয়া', 'উঠিয়া', 'ফিরিয়া', 'অবশেষে', 'কিছুতেই',\n",
        "                                                      'তেমনি', 'তোমাকে', 'নে', 'মাঝে', 'সকল', 'অত্যন্ত', 'একটু',\n",
        "                                                      'একেবারে', 'এক', 'একটা', 'একদিন', 'কথা', 'তারই', 'হয়ে', 'হওয়ায়',\n",
        "                                                      'রয়েছে', 'এমনকি', 'একজন', 'দিয়েছেন', 'একটা', 'হয়নি', 'সাথে',\n",
        "                                                      'হয়েই', 'দিয়ে', 'কেমনে', 'করিয়ে', 'তোরা', 'জন্যে', 'পেয়ে',\n",
        "                                                      'পাওয়া', 'তোর', 'ছাড়া', 'ছাড়াও', 'হওয়ার', 'তোমাদের', 'চেয়ে',\n",
        "                                                      'কথা', 'জানিয়েছে', 'মত', 'অর্থাৎ', 'গিয়েছে', 'জাানিয়ে', 'হয়েছে',\n",
        "                                                      'হিসেবে', 'হওয়া', 'এলো', 'করায়', 'তাঁহারা', 'দেওয়ার', 'হইয়া',\n",
        "                                                      'হয়েছেন', 'তোদের', 'অর্ধভাগে', 'তিনই', 'এসো', 'দেয়', 'এক', 'যায়',\n",
        "                                                      'দিয়েছে', 'চায়', 'হয়েছিল', 'তুই', 'হয়তো', 'হৈতে', 'অনুযায়ী',\n",
        "                                                      'কয়েকটি', 'পাঁচ', 'করিয়া', 'সময়', 'থাকায়'})\n",
        "\n",
        "\t\t# Perform text normalisation\n",
        "\t\t# Reference: https://nbviewer.jupyter.org/url/anoopkunchukuttan.github.io/indic_nlp_library/doc/indic_nlp_examples.ipynb#Text-Normalization\n",
        "\t\tfactory = IndicNormalizerFactory()\n",
        "\t\tnormalizer = factory.get_normalizer(\"bn\", remove_nuktas=False)\n",
        "\t\toutput_text=normalizer.normalize(input_text)\n",
        "\n",
        "\t\t# Remove punctuation from the text.\n",
        "\t\ttext = output_text.strip()\n",
        "\t\ttext = text.replace(\"’\", \"\")\n",
        "\t\ttext = text.replace(\"‘\", \"\")\n",
        "\t\ttext = text.replace('”', \"\")\n",
        "\t\ttext = text.replace(\"“\", \"\")\n",
        "\n",
        "\t\t# Some of the punctuation symbols were obtained from here: https://github.com/paul-pias/Text-Preprocessing-in-Bangla-and-English\n",
        "\t\tclean_text = re.sub(r\"[!\\\"”#$%&’()*+,-./:;<=>?@\\[\\]^_`{|}~–—\\'।॥]+\\ *\", ' ', text)\n",
        "\t\tclean_text = re.sub(r\"[-—]+\\ *\", ' ', clean_text)\n",
        "\t\tclean_text = re.sub(r\"\\n\", '', clean_text)\n",
        "\t\tclean_text = \" \".join(clean_text.split())\n",
        "\n",
        "\t\t# tokenize the cleaned text\n",
        "\t\tterms = clean_text.split(' ')\n",
        "\t\t# Remove stop words\n",
        "\t\tfiltered_sentence = []\n",
        "\t\tfor w in terms:\n",
        "\t\t\tif w not in stop_words:\n",
        "\t\t\t\tfiltered_sentence.append(w)\n",
        "\t\tterms = filtered_sentence\n",
        "\t\treturn terms"
      ],
      "metadata": {
        "id": "Qpw0-zBGzf8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_vectorizer = TfidfVectorizer(tokenizer=document_linearization().pre_process,stop_words=None, \n",
        "                             use_idf=True, \n",
        "                             smooth_idf=True,max_df=0.9)\n",
        "tr_vectorizer.fit(tr_docs)\n",
        "# The below tfidf_matix has the TF-IDF values of all the documents in the corpus. This is a big sparse doc-term matrix.\n",
        "tr_tfidf_matrix  = tr_vectorizer.transform(tr_docs)"
      ],
      "metadata": {
        "id": "WXjw_IckzoWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_vectorizer = TfidfVectorizer(tokenizer=document_linearization().pre_process,stop_words=None, \n",
        "                             use_idf=True, \n",
        "                             smooth_idf=True,max_df=0.9)\n",
        "c_vectorizer.fit(c_docs)\n",
        "# The below tfidf_matix has the TF-IDF values of all the articles in the concept corpus. This is a big sparse matrix.\n",
        "c_tfidf_matrix  = c_vectorizer.transform(c_docs)"
      ],
      "metadata": {
        "id": "-e4UL8QjzoyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_doc_term_matrix = tr_tfidf_matrix.toarray()\n",
        "\"\"\" The rows in tr_doc_term_matrix represents the documents. To find which document is referred to do tr_file_paths[row_number_of_tr_doc_term_matrix]\n",
        "    The columns in tr_doc_term_matrix represents the terms of the vocabulary of the train dataset.\n",
        "\"\"\"\n",
        "c_doc_term_matrix = c_tfidf_matrix.toarray()\n",
        "tr_doc_concept_matrix = np.zeros((len(tr_file_paths),c_doc_term_matrix.shape[0]))"
      ],
      "metadata": {
        "id": "gHVgYKQp0C23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a list of all the unique words in the vocabulary of Train Dataset.\n",
        "tr_feature_names = tr_vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "0Q4MGQAp0DT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a list of all the unique words in the vocabulary of Concept Dataset.\n",
        "c_feature_names = c_vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "wldw-nPA0PGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title For travel sentence embeddings\n",
        "# bert_sent_path = '/content/drive/MyDrive/New_Work/IndicBert_Exp/travdoc_sentESA.pkl'\n",
        "# esa_sent_path = '/content/drive/MyDrive/New_Work/IndicBert_Exp/TravLabse.pkl'"
      ],
      "metadata": {
        "id": "Jm2HeliahFFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title For health sentence embeddings\n",
        "bert_sent_path = '/content/drive/MyDrive/New_Work/IndicBert_Exp/Healthdoc_sentESA.pkl'\n",
        "esa_sent_path = '/content/drive/MyDrive/New_Work/IndicBert_Exp/LaBSE_Exp/LaBSE_Embeddings/HealthLabse.pkl'"
      ],
      "metadata": {
        "id": "N2zdghJzhrcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(bert_sent_path, 'rb') as f:\n",
        "    doc_sentence_ESA= pickle.load(f)"
      ],
      "metadata": {
        "id": "x9KoBA2I0ZH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(esa_sent_path, 'rb') as f:\n",
        "  doc_sentence_BERT = pickle.load(f)"
      ],
      "metadata": {
        "id": "QG6F_AyW0pDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_wt(list1, list2):\n",
        "    \"\"\"\n",
        "    The goal of Document weight function is to assign higher weight\n",
        "    to documents containing more words from the query than documents having fewer query words\n",
        "    :param list1: list of words in query after pre-processing the query.\n",
        "    :param list2: list of words in the document after pre-processing.\n",
        "    :return: Document-Query Overlap score in the range of [0,1].\n",
        "    \"\"\"\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    intersection = float(len(s1.intersection(s2)))\n",
        "    query_wt = float(len(s1))\n",
        "    try:\n",
        "        return intersection / query_wt\n",
        "    except:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "SVM9UK8V1VhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_list = [\"কোভিড মহামারীর দ্বিতীয় ঢেউয়ে আক্রান্তদের মধ্যে নানান জটিলতা\"]"
      ],
      "metadata": {
        "id": "sjRyAQIX2Jhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_args= defaultdict(lambda: 0)"
      ],
      "metadata": {
        "id": "HE6CIhx0UNXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_vectorizer1 = TfidfVectorizer(tokenizer=document_linearization().pre_process1,stop_words=None, \n",
        "                             use_idf=True, \n",
        "                             smooth_idf=True,max_df=0.9)\n",
        "tr_vectorizer1.fit(tr_docs)\n",
        "# The below tfidf_matix has the TF-IDF values of all the documents in the corpus. This is a big sparse doc-term matrix.\n",
        "tr_tfidf_matrix1  = tr_vectorizer1.transform(tr_docs)\n",
        "\n",
        "for  ind, query in enumerate(query_list):\n",
        "  qrs=[query]\n",
        "  qvecs = tr_vectorizer1.transform(qrs)\n",
        "  q_idx = []\n",
        "  for v in qvecs:\n",
        "    q_idx.append(v.indices)\n",
        "  for i in range(0,len(qrs)):\n",
        "    A=sklearn.metrics.pairwise.cosine_similarity(qvecs,tr_tfidf_matrix1)\n",
        "    row_max = A.max(axis=1)\n",
        "    A = A / row_max[:, np.newaxis]\n",
        "    # B = np.zeros(shape=A.shape)\n",
        "    # for j in range(0, tr_tfidf_matrix.shape[0]):\n",
        "    #   B[i][j] = document_wt([tr_feature_names[x] for x in q_idx[i]],[tr_feature_names[x] for x in tr_tfidf_matrix[j, :].nonzero()[1]])\n",
        "    # for i in range(0, len(qrs)):\n",
        "    #   for j in range(0, tr_tfidf_matrix.shape[0]):\n",
        "    #     try:\n",
        "    #       A[i][j] = statistics.harmonic_mean([A[i][j], B[i][j]])\n",
        "    #     except:\n",
        "    #       A[i][j] = 0 \n",
        "    retrieved_docs=[]\n",
        "    matching_scores=[]\n",
        "    for i in range(0,len(qrs)):\n",
        "      x=np.argsort(A[i]).T[::-1][:150]\n",
        "      j=0\n",
        "      for y in x:\n",
        "        if A[i][y]>0:\n",
        "          matching_scores.append(round(A[i][y],4))\n",
        "          retrieved_docs.append(int(tr_file_paths[y].split('/')[-1].replace('.txt','')))\n",
        "          j+=1\n",
        "          doc_args[int(tr_file_paths[y].split('/')[-1].replace('.txt',''))]=dict()\n",
        "          tfidf= defaultdict(lambda: 0)\n",
        "          tfidf['tf-idf']=round(A[i][y],4)\n",
        "          doc_args[int(tr_file_paths[y].split('/')[-1].replace('.txt',''))]=tfidf\n",
        "    print(\"Query: \",qrs[i],\"\\ntf-idf: \",retrieved_docs[0:10])"
      ],
      "metadata": {
        "id": "FIjkWnin1xJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"No. of document retrieved by tf-idf: \",len(retrieved_docs))"
      ],
      "metadata": {
        "id": "qbVu5o-yN4Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matching_scores"
      ],
      "metadata": {
        "id": "VQr4AthW3KTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the BERT Representation of the query.\n"
      ],
      "metadata": {
        "id": "kOxN3CzcYa2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qbert = model.encode(query_list)\n",
        "qbert = np.squeeze(qbert)"
      ],
      "metadata": {
        "id": "tc70MybDvZYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the “best” k(=3) sentence in a document which provides a good proxy for document relevance."
      ],
      "metadata": {
        "id": "5Que4Z4GbXIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for did in doc_args.keys():\n",
        "  best_sents = []\n",
        "  for sent, sent_vec in doc_sentence_BERT[did]:\n",
        "    bert_score = np.dot(sent_vec,qbert)/(norm(sent_vec)*norm(qbert))\n",
        "    best_sents.append(tuple((sent,bert_score)))\n",
        "  best_sents = sorted(best_sents, key=lambda t: t[1], reverse=True)\n",
        "  while(len(best_sents)<3):\n",
        "    best_sents.append(tuple((\"\",0)))\n",
        "  temp_dict=doc_args[did]\n",
        "  temp_dict['S1']=best_sents[0][0]\n",
        "  temp_dict['S1_bert'] = best_sents[0][1]\n",
        "  temp_dict['S2']=best_sents[1][0]\n",
        "  temp_dict['S2_bert'] = best_sents[0][1]\n",
        "  temp_dict['S3']=best_sents[2][0]\n",
        "  temp_dict['S3_bert'] = best_sents[2][1]\n",
        "  doc_args[did] = temp_dict"
      ],
      "metadata": {
        "id": "NMs0FZctv4A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the w_i's using nautral concepts described by humans themselves (by way of ESA)."
      ],
      "metadata": {
        "id": "EKtLRyOTiBpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrs = []\n",
        "for q in query_list:\n",
        "  q=normalizer.normalize(q)\n",
        "  qrs.append(q)\n",
        "qvecs=tr_vectorizer.transform(qrs)\n",
        "Q=qvecs.toarray()\n",
        "q_concept_matrix = np.zeros((len(qrs),c_doc_term_matrix.shape[0]))\n",
        "for i in range (0,len(qrs)):\n",
        "  q_feature_index = qvecs [i,:].nonzero()[1]\n",
        "  q_tfidf_scores = zip(q_feature_index, [qvecs [i, x] for x in q_feature_index])\n",
        "  tfidf_vec = np.zeros((c_doc_term_matrix.shape[0]))\n",
        "  # w represents the term 't'\n",
        "  # s represents the tfidf score of the term 't' in document 'd'.\n",
        "  for w, s in [(tr_feature_names[i], s) for (i, s) in q_tfidf_scores]:\n",
        "    try:\n",
        "      c_word_index = c_vectorizer.vocabulary_[w]\n",
        "      # All the documents in the Concept_Dataset is considered to be a wikipedia concept.\n",
        "      temp_vec = np.zeros((c_doc_term_matrix.shape[0]))\n",
        "      for j in range(0,c_doc_term_matrix.shape[0]):\n",
        "        temp_vec[j] =  c_doc_term_matrix[j][c_word_index]* s\n",
        "      tfidf_vec = tfidf_vec + temp_vec\n",
        "    except:\n",
        "      continue\n",
        "  q_concept_matrix[i] = tfidf_vec \n",
        "  qESA=scipy.sparse.csr_matrix(q_concept_matrix)"
      ],
      "metadata": {
        "id": "PrPkPtrPhIhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_concept_representation(word, tf_idf_score, concept_vec_size):\n",
        "  concept_vec = np.zeros((concept_vec_size))\n",
        "  try:\n",
        "    c_word_index = c_vectorizer.vocabulary_[w]\n",
        "    for j in range(0,c_doc_term_matrix.shape[0]):\n",
        "      concept_vec[j] =  c_doc_term_matrix[j][c_word_index] * tf_idf_score\n",
        "  except:\n",
        "    return scipy.sparse.csr_matrix(np.zeros((concept_vec_size)))\n",
        "  return scipy.sparse.csr_matrix(concept_vec) "
      ],
      "metadata": {
        "id": "x-B2Gza7iWhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute this cell if you dont want sentence explanation with ESA\n",
        "for did in doc_args.keys():\n",
        "  temp_dict=doc_args[did]\n",
        "  for idx, tup in enumerate(doc_sentence_ESA[did]):\n",
        "    if tup[0] == doc_args[did]['S1']:\n",
        "      S1_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "      w1 = sklearn.metrics.pairwise.cosine_similarity(qESA,S1_ESA).item()\n",
        "      temp_dict['w1']=w1\n",
        "    if tup[0] == doc_args[did]['S2']:\n",
        "      S2_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "      w2 = sklearn.metrics.pairwise.cosine_similarity(qESA,S2_ESA).item()\n",
        "      temp_dict['w2']=w2\n",
        "    if tup[0] == doc_args[did]['S3']:\n",
        "      S3_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "      w3 = sklearn.metrics.pairwise.cosine_similarity(qESA,S3_ESA).item()\n",
        "      temp_dict['w3']=w3\n",
        "  doc_args[did] = temp_dict"
      ],
      "metadata": {
        "id": "mKaXtDUcXb2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute this cell if you want sentence explanation with ESA\n",
        "for did in doc_args.keys():\n",
        "  temp_dict=doc_args[did]\n",
        "  for idx, tup in enumerate(doc_sentence_ESA[did]):\n",
        "    try:\n",
        "      if tup[0] == doc_args[did]['S1']:\n",
        "        S1_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "        w1 = sklearn.metrics.pairwise.cosine_similarity(qESA,S1_ESA).item()\n",
        "        temp_dict['w1']=w1\n",
        "        i = tr_dids.index(did)\n",
        "        tr_feature_index = tr_tfidf_matrix [i,:].nonzero()[1]\n",
        "        tr_tfidf_scores = zip(tr_feature_index, [tr_tfidf_matrix [i, x] for x in tr_feature_index])\n",
        "        tfidf_vec = np.zeros((c_doc_term_matrix.shape[0]))\n",
        "        doc_token_tfidf=dict()\n",
        "        for w, s in [(tr_feature_names[i], s) for (i, s) in tr_tfidf_scores]:\n",
        "          doc_token_tfidf[w]=s\n",
        "        word_rel_score=[]\n",
        "        max_word_score = 0.0\n",
        "        for w in document_linearization().pre_process(doc_args[did]['S1']):\n",
        "          try:\n",
        "            s = doc_token_tfidf[w]\n",
        "            word_score = round(sklearn.metrics.pairwise.cosine_similarity(get_concept_representation(w,s,c_doc_term_matrix.shape[0]),qESA)[0][0],4)\n",
        "            max_word_score = max(max_word_score,word_score)\n",
        "            word_rel_score.append([w,word_score])\n",
        "          except:\n",
        "            continue\n",
        "        explainable_terms = []\n",
        "        for word_index in range (0,len(word_rel_score)):\n",
        "          word_rel_score[word_index][1] = round(word_rel_score[word_index][1]/max_word_score,4)\n",
        "          if word_rel_score[word_index][1]> 0.5*max_word_score:\n",
        "            explainable_terms.append(tuple(word_rel_score[word_index]))\n",
        "        explainable_terms = sorted(explainable_terms, key=lambda t: t[1], reverse=True)\n",
        "        temp_dict['S1_ESA']=list(explainable_terms)\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      if tup[0] == doc_args[did]['S2']:\n",
        "        S2_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "        w2 = sklearn.metrics.pairwise.cosine_similarity(qESA,S2_ESA).item()\n",
        "        temp_dict['w2']=w2\n",
        "        i = tr_dids.index(did)\n",
        "        tr_feature_index = tr_tfidf_matrix [i,:].nonzero()[1]\n",
        "        tr_tfidf_scores = zip(tr_feature_index, [tr_tfidf_matrix [i, x] for x in tr_feature_index])\n",
        "        tfidf_vec = np.zeros((c_doc_term_matrix.shape[0]))\n",
        "        doc_token_tfidf=dict()\n",
        "        for w, s in [(tr_feature_names[i], s) for (i, s) in tr_tfidf_scores]:\n",
        "          doc_token_tfidf[w]=s\n",
        "        word_rel_score=[]\n",
        "        max_word_score = 0.0\n",
        "        for w in document_linearization().pre_process(doc_args[did]['S2']):\n",
        "          try:\n",
        "            s = doc_token_tfidf[w]\n",
        "            word_score = round(sklearn.metrics.pairwise.cosine_similarity(get_concept_representation(w,s,c_doc_term_matrix.shape[0]),qESA)[0][0],4)\n",
        "            max_word_score = max(max_word_score,word_score)\n",
        "            word_rel_score.append([w,word_score])\n",
        "          except:\n",
        "            continue\n",
        "        explainable_terms = []\n",
        "        for word_index in range (0,len(word_rel_score)):\n",
        "          word_rel_score[word_index][1] = round(word_rel_score[word_index][1]/max_word_score,4)\n",
        "          if word_rel_score[word_index][1]> 0.5*max_word_score:\n",
        "            explainable_terms.append(tuple(word_rel_score[word_index]))\n",
        "        explainable_terms = sorted(explainable_terms, key=lambda t: t[1], reverse=True)\n",
        "        temp_dict['S2_ESA']=list(explainable_terms)\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      if tup[0] == doc_args[did]['S3']:\n",
        "        S3_ESA = scipy.sparse.csr_matrix(doc_sentence_ESA[did][idx][1])\n",
        "        w3 = sklearn.metrics.pairwise.cosine_similarity(qESA,S3_ESA).item()\n",
        "        temp_dict['w3']=w3\n",
        "        i = tr_dids.index(did)\n",
        "        tr_feature_index = tr_tfidf_matrix [i,:].nonzero()[1]\n",
        "        tr_tfidf_scores = zip(tr_feature_index, [tr_tfidf_matrix [i, x] for x in tr_feature_index])\n",
        "        tfidf_vec = np.zeros((c_doc_term_matrix.shape[0]))\n",
        "        doc_token_tfidf=dict()\n",
        "        for w, s in [(tr_feature_names[i], s) for (i, s) in tr_tfidf_scores]:\n",
        "          doc_token_tfidf[w]=s\n",
        "        word_rel_score=[]\n",
        "        max_word_score = 0.0\n",
        "        for w in document_linearization().pre_process(doc_args[did]['S3']):\n",
        "          try:\n",
        "            s = doc_token_tfidf[w]\n",
        "            word_score = round(sklearn.metrics.pairwise.cosine_similarity(get_concept_representation(w,s,c_doc_term_matrix.shape[0]),qESA)[0][0],4)\n",
        "            max_word_score = max(max_word_score,word_score)\n",
        "            word_rel_score.append([w,word_score])\n",
        "          except:\n",
        "            continue\n",
        "        explainable_terms = []\n",
        "        for word_index in range (0,len(word_rel_score)):\n",
        "          word_rel_score[word_index][1] = round(word_rel_score[word_index][1]/max_word_score,4)\n",
        "          if word_rel_score[word_index][1]> 0.5*max_word_score:\n",
        "            explainable_terms.append(tuple(word_rel_score[word_index]))\n",
        "        explainable_terms = sorted(explainable_terms, key=lambda t: t[1], reverse=True)\n",
        "        temp_dict['S3_ESA']=list(explainable_terms)\n",
        "    except:\n",
        "      pass\n",
        "  doc_args[did] = temp_dict"
      ],
      "metadata": {
        "id": "GAWKZLwEqZPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S2_ESA.shape"
      ],
      "metadata": {
        "id": "1htrh6wCPW3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcualate the final matching score of the document to the query."
      ],
      "metadata": {
        "id": "zdeBs1z78WKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_q_doc_bert(sdoc, alpha, w1, w2, w3, S1_bert, S2_bert, S3_bert, weight):\n",
        "    if weight == \"equal\":\n",
        "      w1=1\n",
        "      w2=1\n",
        "      w3=1\n",
        "    weight_list = [w1, w2, w3]\n",
        "    bert_sent_score = [S1_bert, S2_bert, S3_bert]\n",
        "    sum_score = 0\n",
        "    for s, w in zip(weight_list, bert_sent_score):\n",
        "      sum_score += s * w\n",
        "    doc_score = alpha * sdoc + (1.0 - alpha) * sum_score\n",
        "    return doc_score"
      ],
      "metadata": {
        "id": "xn_041T5nI-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_rank = []\n",
        "try:\n",
        "  for did in doc_args.keys():\n",
        "    final_doc_score = calc_q_doc_bert(sdoc = doc_args[did]['tf-idf'],alpha = 0.5, w1 = doc_args[did]['w1'], w2 = doc_args[did]['w2'], w3 = doc_args[did]['w3'],S1_bert = doc_args[did]['S1_bert'], S2_bert = doc_args[did]['S2_bert'], S3_bert = doc_args[did]['S3_bert'], weight=\"not equal\")\n",
        "    retrieved_rank.append(tuple((did,final_doc_score)))\n",
        "except:\n",
        "  print(\"Some Error\",did)\n",
        "retrieved_rank = sorted(retrieved_rank, key=lambda t: t[1], reverse=True)\n",
        "# print(retrieved_rank[:10])\n",
        "print(\"ESA Weights: \",list(zip(*retrieved_rank[:10]))[0])"
      ],
      "metadata": {
        "id": "8YyZiBZ38JtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_rank = []\n",
        "try:\n",
        "  for did in doc_args.keys():\n",
        "    final_doc_score = calc_q_doc_bert(sdoc = doc_args[did]['tf-idf'],alpha = 0.5, w1 = doc_args[did]['w1'], w2 = doc_args[did]['w2'], w3 = doc_args[did]['w3'],S1_bert = doc_args[did]['S1_bert'], S2_bert = doc_args[did]['S2_bert'], S3_bert = doc_args[did]['S3_bert'], weight=\"equal\")\n",
        "    retrieved_rank.append(tuple((did,final_doc_score)))\n",
        "except:\n",
        "  print(\"Some Error\",did)\n",
        "retrieved_rank = sorted(retrieved_rank, key=lambda t: t[1], reverse=True)\n",
        "# print(retrieved_rank[:10])\n",
        "print(\"Equal Weights: \",list(zip(*retrieved_rank[:10]))[0])"
      ],
      "metadata": {
        "id": "68u5zfie7wRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for did,score in retrieved_rank[0:3]:\n",
        "  print(\"Doc id: \",did)\n",
        "  print(\"S1: \",doc_args[did]['S1'])\n",
        "  print(\"S2: \",doc_args[did]['S2'])\n",
        "  print(\"S3: \",doc_args[did]['S3'])\n",
        "  print()"
      ],
      "metadata": {
        "id": "vnqmBk7akWSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AhZ_Ho-ArxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}